{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Article Identify Test.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"nt0nDFYsJvr3","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")  # Don't want to see the warnings in the notebook"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DWGepHuHJ2Ua","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0055a8b9-f170-4060-8485-5b35caf8a2b1","executionInfo":{"status":"ok","timestamp":1568418981639,"user_tz":-60,"elapsed":5933,"user":{"displayName":"Mert Yilmaz Ertugrul (Student)","photoUrl":"","userId":"05571684688610331947"}}},"source":["!pip install PyPDF2"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.6/dist-packages (1.26.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7PouGPaTHeXz","colab_type":"code","colab":{}},"source":["import joblib\n","import pandas as pd\n","import numpy as np\n","\n","import os\n","import sys\n","import spacy\n","import re\n","import os\n","from math import sqrt\n","import PyPDF2\n","from tqdm import tqdm_notebook\n","import joblib\n","from sklearn import preprocessing\n","from SentenceParcerForTenses import tense_predictorH, sentence_parser, sentence_adder\n","import codecs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"USYrbwpdIWLb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d41e2b48-c89d-40d0-bb8d-5154d01c2dff","executionInfo":{"status":"ok","timestamp":1568418981641,"user_tz":-60,"elapsed":1523,"user":{"displayName":"Mert Yilmaz Ertugrul (Student)","photoUrl":"","userId":"05571684688610331947"}}},"source":["\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import sys\n","sys.path.append('/content/gdrive/My Drive/Tooth Identifier/')"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NxbDNsF4H4R3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"3b9904d9-557e-4c42-9cb4-24bcf3f081bc","executionInfo":{"status":"error","timestamp":1568420739107,"user_tz":-60,"elapsed":545,"user":{"displayName":"Mert Yilmaz Ertugrul (Student)","photoUrl":"","userId":"05571684688610331947"}}},"source":["model = joblib.load('/content/gdrive/My Drive/Tooth Identifier/article_smote_rf_model')"],"execution_count":42,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-2879f7dc8222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/Tooth Identifier/article_smote_rf_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/Tooth Identifier/article_smote_rf_model'"]}]},{"cell_type":"code","metadata":{"id":"_WghLSkyIn5k","colab_type":"code","colab":{}},"source":["df = pd.read_csv('gdrive/My Drive/Tooth Identifier/SentenceToTenses.csv', sep=',')\n","df.drop(['Unnamed: 0'], inplace=True, axis=1)\n","df_columns = df.columns.tolist()\n","columnsForDummies = df_columns[1:len(df_columns) - 1]\n","dfd = pd.get_dummies(df, drop_first=True, columns=columnsForDummies)\n","label_encoder = preprocessing.LabelEncoder()\n","df['tense'] = label_encoder.fit_transform(df['tense'])\n","dt = joblib.load('gdrive/My Drive/Tooth Identifier/tense_dt_model')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMXAMQy8KISY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"2130b34d-2721-4cf2-d34a-619785665afb","executionInfo":{"status":"ok","timestamp":1568404167000,"user_tz":-60,"elapsed":216023,"user":{"displayName":"Mert Yilmaz Ertugrul (Student)","photoUrl":"","userId":"05571684688610331947"}}},"source":["import spacy.cli\n","nlp = spacy.cli.download('en_core_web_lg')\n","import en_core_web_lg\n","nlp = en_core_web_lg.load()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_lg')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tsFmwf0gKQ0-","colab_type":"code","colab":{}},"source":["advance_vocab_df = pd.read_csv('gdrive/My Drive/Tooth Identifier/vocabulary.csv')  # Advanced words\n","common_vocab_df = pd.read_csv('gdrive/My Drive/Tooth Identifier/common.csv')  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjBaPy__KUN_","colab_type":"code","colab":{}},"source":["def syllables(word):\n","    \"\"\"\n","    returns the number of syllables of a given word\n","    :param word: single word in string format\n","    :return: number of syllables\n","    \"\"\"\n","    syllable_count = 0\n","    vowels = 'aeiouy'\n","    if word[0] in vowels:\n","        syllable_count += 1\n","    for index in range(1, len(word)):\n","        if word[index] in vowels and word[index - 1] not in vowels:\n","            syllable_count += 1\n","    if word.endswith('e'):\n","        syllable_count -= 1\n","    if word.endswith('le') and len(word) > 2 and word[-3] not in vowels:\n","        syllable_count += 1\n","    if syllable_count == 0:\n","        syllable_count += 1\n","    return syllable_count\n","\n","\n","def check_verb(token):\n","    \"\"\"Check verb type given spacy token\n","    :param token: single verb in a sting format\n","    :return: linguistic type of verb\n","    \"\"\"\n","    indirect_object = False\n","    direct_object = False\n","    if token.tag_ == 'BES':\n","        return 'TOBEVERB'\n","    for item in token.children:\n","        if item.dep_ == \"iobj\" or item.dep_ == \"pobj\":\n","            indirect_object = True\n","        if item.dep_ == \"dobj\" or item.dep_ == \"dative\":\n","            direct_object = True\n","    if indirect_object and direct_object:\n","        return 'DITRANVERB'\n","    elif direct_object and not indirect_object:\n","        return 'TRANVERB'\n","    elif not direct_object and not indirect_object:\n","        return 'INTRANVERB'\n","    else:\n","        return 'VERB'\n","\n","\n","def word_finder(vocab, word):\n","    \"\"\"\n","    Finds the verb in a given vocabulary\n","    :param vocab: vocabulary in dataformat\n","    :param word: word to look in vocabulary list\n","    :return: boolean, whether the word in the vocabulary or not\n","    \"\"\"\n","    doc = nlp(word)\n","    if doc[0].is_punct is False:\n","        try:\n","            result = vocab.word.str.contains(r'(?:\\s|^)' + word + '(?:\\s|$)').any()\n","        except:\n","            result = False\n","        if result:\n","            return True\n","        else:\n","            return False\n","    else:\n","        return False\n","\n","\n","def is_advanced(word):\n","    \"\"\"\n","    Checks the word is in advance vocabulary\n","    :return: boolean\n","    :param word: word to look at\n","    \"\"\"\n","    return word_finder(advance_vocab_df, word)\n","\n","\n","def is_common(word):\n","    \"\"\"\n","    Checks the word is in common vocabulary\n","    :param word: word to look at\n","    :return: boolean\n","    \"\"\"\n","    return word_finder(common_vocab_df, word)\n","\n","\n","def feature_computer(doc):\n","    \"\"\"\n","    Creates and calculate features of the article\n","    :param doc: tokenized article, spacy object\n","    :return: dictionary of the features\n","    \"\"\"\n","    result = {}\n","    # Linguistic\n","    c_syllable = 0\n","    c_monosylable = 0\n","    c_disyllable = 0\n","    c_complex_word = 0\n","    c_word = 0\n","    c_sentence = 0\n","    c_character = 0\n","    c_advance = 0\n","    c_common = 0\n","    c_paragraph = 0\n","    # Word Usage\n","    c_ditransverb = 0\n","    c_transverb = 0\n","    c_intransverb = 0\n","    c_verb = 0\n","    c_conjunction = 0\n","    c_auxverb = 0\n","    c_noun = 0\n","    c_punct = 0\n","    c_adjective = 0\n","    c_adposition = 0\n","    c_adverb = 0\n","    c_pronoun = 0\n","    c_unknown = 0\n","    # Tenses\n","    c_present_simple = 0\n","    c_past_simple = 0\n","    c_future_simple = 0\n","    c_past_progressive = 0\n","    c_present_progressive = 0\n","    c_future_progressive = 0\n","    c_present_perfect = 0\n","    c_past_perfect = 0\n","    c_future_perfect = 0\n","    c_present_perfect_progressive = 0\n","    c_past_perfect_progressive = 0\n","    c_future_perfect_progressive = 0\n","    tense_passed = False\n","    c_tense_passed = 1\n","\n","    for sent in tqdm_notebook(doc.sents, total=len(list(doc.sents))):\n","        c_sentence += 1\n","        try:\n","            parsed = sentence_parser(sent.text, print_sent=False)\n","        except Exception as e:\n","            exc_type, exc_obj, exc_tb = sys.exc_info()\n","            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n","            print(exc_type, fname, exc_tb.tb_lineno, ' - ', c_tense_passed)\n","            print(str(e))\n","            tense_passed = True\n","            c_tense_passed += 1\n","            pass\n","        if tense_passed is False:\n","            tense = tense_predictorH(parsed, df, dfd, dt, label_encoder)\n","            if tense == 'present simple':\n","                c_present_simple += 1\n","            if tense == 'past simple':\n","                c_past_simple += 1\n","            if tense == 'future simple':\n","                c_future_simple += 1\n","            if tense == 'past progressive':\n","                c_past_progressive += 1\n","            if tense == 'present progressive':\n","                c_present_progressive += 1\n","            if tense == 'future progressive':\n","                c_future_progressive += 1\n","            if tense == 'present perfect':\n","                c_past_perfect += 1\n","            if tense == 'present perfect progressive':\n","                c_present_perfect_progressive += 1\n","            if tense == 'past perfect progressive':\n","                c_past_perfect_progressive += 1\n","            if tense == 'future perfect progressive':\n","                c_future_perfect_progressive += 1\n","            if tense == 'future perfect':\n","                c_future_perfect += 1\n","            if tense == 'past perfect':\n","                c_past_perfect += 1\n","\n","        for token in sent:\n","            if token.is_stop is False and token.is_punct is False:\n","                # Number of words\n","                c_word += 1\n","                # Number of characters\n","                c_character += len(token.text)\n","                # Syllables\n","                num_syllable = syllables(token.text)\n","                if num_syllable == 1:\n","                    c_monosylable += 1\n","                if num_syllable == 2:\n","                    c_disyllable += 1\n","                if num_syllable >= 3:\n","                    c_complex_word += 1\n","                c_syllable += num_syllable\n","                # Advance Word\n","                if is_advanced(token.text):\n","                    c_advance += 1\n","                # Common Word\n","                if is_common(token.text):\n","                    c_common += 1\n","                # Paragraph\n","                if token.is_space:\n","                    c_paragraph += 1\n","                # Word Usage\n","                if token.pos_ == 'VERB':\n","                    verb = check_verb(token)\n","                    if verb == 'DITRANVERB':\n","                        c_ditransverb += 1\n","                    elif verb == 'TRANVERB':\n","                        c_transverb += 1\n","                    elif verb == 'INTRANVERB':\n","                        c_intransverb += 1\n","                    c_verb += 1\n","                elif token.tag_ == 'CC' or token.pos_ == 'CCONJ' or token.pos_ == 'CONJ' or token.pos_ == 'SCONJ':\n","                    c_conjunction += 1\n","                elif token.pos_ == 'AUX':\n","                    c_auxverb += 1\n","                elif token.pos_ == 'NOUN':\n","                    c_noun += 1\n","                elif token.pos_ == 'PUNCT':\n","                    c_punct += 1\n","                elif token.pos_ == 'ADJ':\n","                    c_adjective += 1\n","                elif token.pos_ == 'ADP':\n","                    c_adposition += 1\n","                elif token.pos_ == 'ADV':\n","                    c_adverb += 1\n","                elif token.pos_ == 'PRON' or token.pos_ == 'PROPN':\n","                    c_pronoun += 1\n","                elif token.pos_ == 'X':\n","                    c_unknown += 1\n","        c_tense_passed = False\n","\n","    # Score calculating\n","    dale_chall_score = (0.1579 * (100 - (c_common / c_word * 100))) + (0.0496 * (c_word / c_sentence))\n","    Flesch_Reading_Ease_Score = 206.835 - (1.015 * c_word / c_sentence) - (84.6 * c_syllable / c_word)\n","    new_flesch_reading_ease_score = (1.599 * c_monosylable / c_word * 100) - (1.015 * c_word / c_sentence) - 31.517\n","    gunning_fog_score = 0.4 * ((c_word / c_sentence) + ((c_complex_word/c_word) * 100))\n","    smog_score = 3 + sqrt((c_disyllable + c_complex_word) / c_word * 30)\n","    forcast_score = 20 - (((c_monosylable / c_word) * 150)/10)\n","    ari_score = 4.71 * (c_character / c_word) + (0.5 * (c_word / c_sentence)) - 21.43\n","    coleman_liau_score = (0.0588 * c_character / c_word * 100) - (0.296 * (c_sentence / c_word) * 100) - 15.8\n","    lix_score = (c_word / c_sentence) + (c_complex_word / c_word * 100)\n","    rix_score = ((c_complex_word / c_word) * 100) + (c_word / c_sentence)\n","    powers_sumner_kearl = (0.0778 * (c_word / c_sentence)) + (0.0455 * (c_syllable/c_word)*100) + 2.7971\n","    spache_score = (0.121 * c_word / c_sentence) + (0.082 * (100 - (c_common / c_word) * 100)) + 0.659\n","    linsear_write = (((c_disyllable * 2) + (c_complex_word * 3) / c_word) * 100)/10000\n","\n","    # Linguistic Features to Dictionary\n","\n","    result['Number of Sentence'] = c_sentence\n","    result['Number of Words'] = c_word\n","    result['Number of Paragraph'] = c_paragraph\n","    try:\n","        result['Words per Sentence (%)'] = c_word / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Words per Sentence (%)'] = 0\n","    result['Number of Characters'] = c_character\n","    try:\n","        result['Character per Words (%)'] = c_character / c_word * 100\n","    except ZeroDivisionError:\n","        result['Character per Words (%)'] = 0\n","    try:\n","        result['Sentence per Paragraph (%)'] = c_sentence / c_paragraph * 100\n","    except ZeroDivisionError:\n","        result['Sentence per Paragraph (%)'] = 0\n","    result['Number of Sylables'] = c_syllable\n","    try:\n","        result['Syllables per Word (%)'] = c_syllable / c_word * 100\n","    except ZeroDivisionError:\n","        result['Syllables per Word (%)'] = 0\n","    result['Number of Monosyllable'] = c_monosylable\n","    try:\n","        result['Monosyllable per Word (%)'] = c_monosylable / c_word * 100\n","    except ZeroDivisionError:\n","        result['Monosyllable per Word (%)'] = 0\n","    try:\n","        result['Disyllable per Word (%)'] = c_disyllable / c_word * 100\n","    except ZeroDivisionError:\n","        result['Disyllable per Word (%)'] = 0\n","    result['Number of Complex Words'] = c_complex_word\n","    try:\n","        result['Complex Words per Word (%)'] = c_complex_word / c_word * 100\n","    except ZeroDivisionError:\n","        result['Complex Words per Word (%)'] = 0\n","    result['Number of Advance Words'] = c_advance\n","    try:\n","        result['Advance Words per Words (%)'] = c_advance / c_word * 100\n","    except ZeroDivisionError:\n","        result['Advance Words per Words (%)'] = 0\n","    result['Number of Common Words'] = c_common\n","    try:\n","        result['Common Words per Words (%)'] = c_common / c_word * 100\n","    except ZeroDivisionError:\n","        result['Common Words per Words (%)'] = 0\n","    try:\n","        result['Verbs per Words (%)'] = c_verb / c_word * 100\n","    except ZeroDivisionError:\n","        result['Verbs per Words (%)'] = 0\n","    try:\n","        result['Verbs per Sentences (%)'] = c_verb / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Verbs per Sentence (%)'] = 0\n","    try:\n","        result['Ditransverbs per Sentences (%)'] = c_ditransverb / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Ditransverbs per Verbs (%)'] = 0\n","    try:\n","        result['Transverbs per Sentences (%)'] = c_transverb / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Transverbs per Sentences (%)'] = 0\n","    try:\n","        result['Intransitverbs per Sentences (%)'] = c_intransverb / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Intransitverbs per Sentences (%)'] = 0\n","    try:\n","        result['Auxverbs per Sentences (%)'] = c_auxverb / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Auxverbs per Sentences (%)'] = 0\n","    try:\n","        result['Conjuctions per Sentences (%)'] = c_conjunction / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Conjuctions per Sentences (%)'] = 0\n","    try:\n","        result['Nouns per Words (%)'] = c_noun / c_word * 100\n","    except ZeroDivisionError:\n","        result['Nouns per Words (%)'] = 0\n","    try:\n","        result['Nouns per Sentence (%)'] = c_noun / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Nouns per Sentence (%)'] = 0\n","    try:\n","        result['Puctuation per Sentence'] = c_punct / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Puctuation per Sentence'] = 0\n","    try:\n","        result['Adjective per Word (%)'] = c_adjective / c_word * 100\n","    except ZeroDivisionError:\n","        result['Adjective per Word (%)'] = 0\n","    try:\n","        result['Adjective per Sentence (%)'] = c_adjective / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Adjective per Sentence (%)'] = 0\n","    try:\n","        result['Adpositions per Sentences (%)'] = c_adposition / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Adpositions per Sentences (%)'] = 0\n","    try:\n","        result['Adverb per Sentence (%)'] = c_adverb / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Adverb per Sentence (%)'] = 0\n","    try:\n","        result['Pronoun per Sentence (%)'] = c_pronoun / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Pronoun per Sentence (%)'] = 0\n","    result['Number of Unknown Words'] = c_unknown\n","    try:\n","        result['Unknown Words per Words (%)'] = c_unknown / c_word * 100\n","    except ZeroDivisionError:\n","        result['Unknown Words per Words (%)'] = 0\n","\n","    # Scores to dictionary\n","    result['Dale Chall Score'] = dale_chall_score\n","    result['Flesch Reading Ease Score'] = Flesch_Reading_Ease_Score\n","    result['New Flesch Reading Ease Score'] = new_flesch_reading_ease_score\n","    result['Gunning Fog Score'] = gunning_fog_score\n","    result['SMOG Score'] = smog_score\n","    result['FORCAST Score'] = forcast_score\n","    result['ARI Score'] = ari_score\n","    result['Coleman Liau Score'] = coleman_liau_score\n","    result['LIX Score'] = lix_score\n","    result['RIX Score'] = rix_score\n","    result['Powers Sumner Kearl'] = powers_sumner_kearl\n","    result['Spache Score'] = spache_score\n","    result['Linsear Write'] = linsear_write\n","\n","    # Tenses to Dictionary\n","    try:\n","        result['Future Perfect Progressive (%)'] = c_future_perfect_progressive / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Future Perfect Progressive  (%)'] = 0\n","    try:\n","        result['Past Simple (%)'] = c_past_simple / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Past Simple (%)'] = 0\n","    try:\n","        result['Present Perfect Progressive (%)'] = c_present_perfect_progressive / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Present Perfect Progressive (%)'] = 0\n","    try:\n","        result['Future Progressive (%)'] = c_future_progressive / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Future Progressive (%)'] = 0\n","    try:\n","        result['Past Perfect Progressive (%)'] = c_past_perfect_progressive / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Past Perfect Progressive (%)'] = 0\n","    try:\n","        result['Past Progressive (%)'] = c_past_progressive / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Past Progressive (%)'] = 0\n","    try:\n","        result['Future Simple (%)'] = c_future_simple / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Future Simple (%)'] = 0\n","    try:\n","        result['Future Perfect (%)'] = c_future_perfect / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Future Perfect (%)'] = 0\n","    try:\n","        result['Present Simple (%)'] = c_present_simple / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Present Simple (%)'] = 0\n","    try:\n","        result['Past Perfect (%)'] = c_past_perfect / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Past Perfect (%)'] = 0\n","    try:\n","        result['Present Progressive (%)'] = c_present_progressive / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Present Progressive (%)'] = 0\n","    try:\n","        result['Present Perfect (%)'] = c_present_perfect / c_sentence * 100\n","    except ZeroDivisionError:\n","        result['Present Perfect (%)'] = 0\n","\n","    return result\n","\n","\n","def feature_computer_df(dic):\n","    \"\"\"\n","    Creates dataframe of the feature dictionary\n","    :param dic: Features of the article in dictionary format\n","    :return: pandas dataframe\n","    \"\"\"\n","    dictionary = feature_computer(dic)\n","    return pd.DataFrame(dictionary, index=[0])\n","\n","\n","def add_article(name, text, level, csv_file=None, is_pdf=False, num_col=False, print_it=True):\n","    \"\"\"\n","    Insert any sentence with proper tense tag to .csv file\n","    :param is_pdf: True if the text is pdf\n","    :param name: Name of the article\n","    :param print_it: If true prints features of the document\n","    :param num_col: If there is a column for row numbers in the csv files make it True\n","    :param text: Text file to added\n","    :param level: level of the sentence\n","    :param csv_file: .csv file to save to\n","    \"\"\"\n","    if is_pdf is False:\n","        with codecs.open(text, 'r', encoding='utf-8',\n","                         errors='ignore') as myfile:\n","            data = str(myfile.read()).replace('.', ' . ')\n","            data = data.replace('-', '')\n","            doc = nlp(data)\n","    else:\n","        pdf_file_obj = open(text, 'rb')\n","        pdf_reader = PyPDF2.PdfFileReader(pdf_file_obj)\n","\n","        text = ''\n","\n","        for page in range(pdf_reader.numPages):\n","            page_obj = pdf_reader.getPage(page)\n","            text += page_obj.extractText()\n","        doc = nlp(text)\n","\n","    new_article_df = feature_computer_df(doc)\n","    new_article_df['Level'] = level\n","    new_article_df.insert(0, 'Name', name)\n","    data_df = pd.read_csv(csv_file)\n","    if num_col:\n","        data_df = df.drop(df.columns[0], axis=1)  # drop number column\n","    data_df = data_df.append(new_article_df)\n","    data_df.to_csv(csv_file, encoding='utf-8', index=False)\n","    if print_it:\n","        display('New data frame added')\n","        display(data_df.tail(1))\n","        # print(data_df.info())\n","    else:\n","        print(df.shape)\n","\n","def get_article_df(name, text):\n","    with codecs.open(text, 'r', encoding='utf-8',\n","                         errors='ignore') as myfile:\n","            data = str(myfile.read()).replace('.', ' . ')\n","            data = data.replace('-', '')\n","            doc = nlp(data)\n","    new_article_df = feature_computer_df(feature_computer(doc))\n","    new_article_df.insert(0, 'Name', name)\n","    \n","    return new_article_df\n","\n","    \n","def get_article_prediction(dataframe, model):\n","    dataframe.drop(['Number of Sentence','Number of Words','Number of Paragraph','Number of Characters','Number of Sylables','Number of Monosyllable',\\\n","              'Number of Complex Words','Number of Advance Words','Number of Common Words','Auxverbs per Sentences (%)',\\\n","        'Verbs per Words (%)','Sentence per Paragraph (%)','Dale Chall Score', 'Flesch Reading Ease Score',\\\n","       'New Flesch Reading Ease Score', 'Gunning Fog Score', 'SMOG Score',\\\n","       'FORCAST Score', 'ARI Score', 'Coleman Liau Score', 'LIX Score',\\\n","       'RIX Score', 'Powers Sumner Kearl', 'Spache Score','Puctuation per Sentence', 'Adjective per Word (%)','Adpositions per Sentences (%)',\\\n","        'Conjuctions per Sentences (%)','Nouns per Words (%)','Number of Unknown Words','Disyllable per Word (%)',\\\n","        'Transverbs per Sentences (%)','Intransitverbs per Sentences (%)',\\\n","         'Verbs per Sentences (%)','Adjective per Sentence (%)','Pronoun per Sentence (%)','Unknown Words per Words (%)',\\\n","         'Future Perfect (%)','Future Progressive (%)','Future Perfect (%)','Present Perfect (%)'], inplace=True, axis=1)\n","    \n","    X = np.array(dataframe.drop(['Name'], axis=1))\n","    prediction = model.predict(X)\n","    return prediction\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Phe1bW0YKn13","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":358},"outputId":"2facc316-00c4-49c6-dd89-5809d9e8b85c","executionInfo":{"status":"error","timestamp":1568412843473,"user_tz":-60,"elapsed":333761,"user":{"displayName":"Mert Yilmaz Ertugrul (Student)","photoUrl":"","userId":"05571684688610331947"}}},"source":["article = 'Economist 6th 20.txt'\n","folder = '/content/gdrive/My Drive/Tooth Identifier/Books/Economist/'\n","df_predict = get_article_df('Economist 6th 20',folder + article)"],"execution_count":36,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fbba1396d8c24ed597cfbe6a8ad666e7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=970), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-aa14cf13407b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Economist 6th 20.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/gdrive/My Drive/Tooth Identifier/Books/Economist/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_article_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Economist 6th 20'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-35-2a45cb2df70d>\u001b[0m in \u001b[0;36mget_article_df\u001b[0;34m(name, text)\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mnew_article_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_computer_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_computer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m     \u001b[0mnew_article_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-35-2a45cb2df70d>\u001b[0m in \u001b[0;36mfeature_computer_df\u001b[0;34m(dic)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \"\"\"\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_computer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-35-2a45cb2df70d>\u001b[0m in \u001b[0;36mfeature_computer\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mc_tense_passed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mc_sentence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'sents'"]}]},{"cell_type":"code","metadata":{"id":"NGxqyt2mnpj8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"3012b41e-a400-48b6-fbbc-8ea6c27b2dac","executionInfo":{"status":"ok","timestamp":1568411686916,"user_tz":-60,"elapsed":890,"user":{"displayName":"Mert Yilmaz Ertugrul (Student)","photoUrl":"","userId":"05571684688610331947"}}},"source":["df_predict.columns"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Words per Sentence (%)', 'Character per Words (%)',\n","       'Syllables per Word (%)', 'Monosyllable per Word (%)',\n","       'Complex Words per Word (%)', 'Advance Words per Words (%)',\n","       'Common Words per Words (%)', 'Ditransverbs per Sentences (%)',\n","       'Nouns per Sentence (%)', 'Adverb per Sentence (%)', 'Linsear Write',\n","       'Future Perfect Progressive (%)', 'Past Simple (%)',\n","       'Present Perfect Progressive (%)', 'Past Perfect Progressive (%)',\n","       'Past Progressive (%)', 'Future Simple (%)', 'Present Simple (%)',\n","       'Past Perfect (%)', 'Present Progressive (%)'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"POckkyU9jpDE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":327},"outputId":"39852845-edb4-40f7-8d2e-65e15a157d04","executionInfo":{"status":"error","timestamp":1568411638187,"user_tz":-60,"elapsed":1226,"user":{"displayName":"Mert Yilmaz Ertugrul (Student)","photoUrl":"","userId":"05571684688610331947"}}},"source":["dfa = df_predict.drop(['Number of Sentence','Number of Words','Number of Paragraph','Number of Characters','Number of Sylables','Number of Monosyllable',\\\n","                       'Number of Complex Words','Number of Advance Words','Number of Common Words','Auxverbs per Sentences (%)',\\\n","                       'Verbs per Words (%)','Sentence per Paragraph (%)','Dale Chall Score', 'Flesch Reading Ease Score',\\\n","                       'New Flesch Reading Ease Score', 'Gunning Fog Score', 'SMOG Score',\\\n","                       'FORCAST Score', 'ARI Score', 'Coleman Liau Score', 'LIX Score',\\\n","                       'RIX Score', 'Powers Sumner Kearl', 'Spache Score','Puctuation per Sentence', 'Adjective per Word (%)','Adpositions per Sentences (%)',\\\n","                       'Conjuctions per Sentences (%)','Nouns per Words (%)','Number of Unknown Words','Disyllable per Word (%)',\\\n","                       'Transverbs per Sentences (%)','Intransitverbs per Sentences (%)',\\\n","                       'Verbs per Sentences (%)','Adjective per Sentence (%)','Pronoun per Sentence (%)','Unknown Words per Words (%)',\\\n","                       'Future Perfect (%)','Future Progressive (%)','Future Perfect (%)','Present Perfect (%)'], inplace=True, axis=1)"],"execution_count":32,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-269b1b23d803>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Number of Sentence'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Paragraph'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Characters'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Sylables'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Monosyllable'\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0;34m'Number of Complex Words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Advance Words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Common Words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Auxverbs per Sentences (%)'\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0;34m'Verbs per Words (%)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Sentence per Paragraph (%)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Dale Chall Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Flesch Reading Ease Score'\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0;34m'New Flesch Reading Ease Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gunning Fog Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SMOG Score'\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0;34m'FORCAST Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ARI Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Coleman Liau Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LIX Score'\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0;34m'RIX Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Powers Sumner Kearl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Spache Score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Puctuation per Sentence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Adjective per Word (%)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Adpositions per Sentences (%)'\u001b[0m\u001b[0;34m,\u001b[0m                       \u001b[0;34m'Conjuctions per Sentences (%)'\u001b[0m\u001b[0;34m,\u001b...\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4964\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4965\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4966\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4967\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['Number of Sentence' 'Number of Words' 'Number of Paragraph'\\n 'Number of Characters' 'Number of Sylables' 'Number of Monosyllable'\\n 'Number of Complex Words' 'Number of Advance Words'\\n 'Number of Common Words' 'Auxverbs per Sentences (%)'\\n 'Verbs per Words (%)' 'Sentence per Paragraph (%)' 'Dale Chall Score'\\n 'Flesch Reading Ease Score' 'New Flesch Reading Ease Score'\\n 'Gunning Fog Score' 'SMOG Score' 'FORCAST Score' 'ARI Score'\\n 'Coleman Liau Score' 'LIX Score' 'RIX Score' 'Powers Sumner Kearl'\\n 'Spache Score' 'Puctuation per Sentence' 'Adjective per Word (%)'\\n 'Adpositions per Sentences (%)' 'Conjuctions per Sentences (%)'\\n 'Nouns per Words (%)' 'Number of Unknown Words' 'Disyllable per Word (%)'\\n 'Transverbs per Sentences (%)' 'Intransitverbs per Sentences (%)'\\n 'Verbs per Sentences (%)' 'Adjective per Sentence (%)'\\n 'Pronoun per Sentence (%)' 'Unknown Words per Words (%)'\\n 'Future Perfect (%)' 'Future Progressive (%)' 'Future Perfect (%)'\\n 'Present Perfect (%)'] not found in axis\""]}]},{"cell_type":"code","metadata":{"id":"5Vm5r008UX15","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":327},"outputId":"f277c43a-dc80-4a66-bbbe-010a07a15e53","executionInfo":{"status":"error","timestamp":1568411400813,"user_tz":-60,"elapsed":532,"user":{"displayName":"Mert Yilmaz Ertugrul (Student)","photoUrl":"","userId":"05571684688610331947"}}},"source":["get_article_prediction(df_predict,model)"],"execution_count":29,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-e9414e2b1166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_article_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-28-c7b7465d28b2>\u001b[0m in \u001b[0;36mget_article_prediction\u001b[0;34m(dataframe, model)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_article_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m     \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Number of Sentence'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Paragraph'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Characters'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Sylables'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Monosyllable'\u001b[0m\u001b[0;34m,\u001b[0m              \u001b[0;34m'Number of Complex Words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Advance Words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Common Words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Auxverbs per Sentences (%)'\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;34m'Verbs per Words (%)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Sentence per Paragraph (%)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Dale Chall Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Flesch Reading Ease Score'\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;34m'New Flesch Reading Ease Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gunning Fog Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SMOG Score'\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;34m'FORCAST Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ARI Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Coleman Liau Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LIX Score'\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;34m'RIX Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Powers Sumner Kearl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Spache Score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Puctuation per Sentence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Adjective per Word (%)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Adpositions per Sentences (%)'\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;34m'Conjuctions per Sentences (%)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Nouns per Words (%)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Number of Unknown Words'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Disyll...\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4964\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4965\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4966\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4967\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['Number of Sentence' 'Number of Words' 'Number of Paragraph'\\n 'Number of Characters' 'Number of Sylables' 'Number of Monosyllable'\\n 'Number of Complex Words' 'Number of Advance Words'\\n 'Number of Common Words' 'Auxverbs per Sentences (%)'\\n 'Verbs per Words (%)' 'Sentence per Paragraph (%)' 'Dale Chall Score'\\n 'Flesch Reading Ease Score' 'New Flesch Reading Ease Score'\\n 'Gunning Fog Score' 'SMOG Score' 'FORCAST Score' 'ARI Score'\\n 'Coleman Liau Score' 'LIX Score' 'RIX Score' 'Powers Sumner Kearl'\\n 'Spache Score' 'Puctuation per Sentence' 'Adjective per Word (%)'\\n 'Adpositions per Sentences (%)' 'Conjuctions per Sentences (%)'\\n 'Nouns per Words (%)' 'Number of Unknown Words' 'Disyllable per Word (%)'\\n 'Transverbs per Sentences (%)' 'Intransitverbs per Sentences (%)'\\n 'Verbs per Sentences (%)' 'Adjective per Sentence (%)'\\n 'Pronoun per Sentence (%)' 'Unknown Words per Words (%)'\\n 'Future Perfect (%)' 'Future Progressive (%)' 'Future Perfect (%)'\\n 'Present Perfect (%)'] not found in axis\""]}]},{"cell_type":"code","metadata":{"id":"zbL46o8qgPov","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}