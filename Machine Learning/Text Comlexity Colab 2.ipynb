{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Articles by Difficulty Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imorting Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJaT6bx-h8oN"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Don't want to see the warnings in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cSA0RPbtiL9P"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 85096,
     "status": "ok",
     "timestamp": 1568371998833,
     "user": {
      "displayName": "Mert Yilmaz Ertugrul (Student)",
      "photoUrl": "",
      "userId": "05571684688610331947"
     },
     "user_tz": -60
    },
    "id": "qCQiQBnGiNtO",
    "outputId": "eddaeb07-5d05-4931-8110-2d43bc87bb84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86808,
     "status": "ok",
     "timestamp": 1568372000570,
     "user": {
      "displayName": "Mert Yilmaz Ertugrul (Student)",
      "photoUrl": "",
      "userId": "05571684688610331947"
     },
     "user_tz": -60
    },
    "id": "PE-SLJCViPUE",
    "outputId": "a809113d-9197-4171-8c8f-d3208b3d0e38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/content/gdrive/Computers': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls /content/gdrive/Computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbGplVeOkFud"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/gdrive/My Drive/Tooth Identifier/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 92725,
     "status": "ok",
     "timestamp": 1568372006568,
     "user": {
      "displayName": "Mert Yilmaz Ertugrul (Student)",
      "photoUrl": "",
      "userId": "05571684688610331947"
     },
     "user_tz": -60
    },
    "id": "zQKNQVXBk6wI",
    "outputId": "d195f791-0e42-4127-d310-026723359a9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 5.0MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
      "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-cp36-none-any.whl size=61085 sha256=3b703a3ac2f1c1bc897a092a9c22e97f6360fa8417cfac1a61a574722f1a363b\n",
      "  Stored in directory: /root/.cache/pip/wheels/53/84/19/35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
      "Successfully built PyPDF2\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-1.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Spi297pLkxxn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from math import sqrt\n",
    "import PyPDF2\n",
    "from tqdm import tqdm_notebook\n",
    "import joblib\n",
    "from sklearn import preprocessing\n",
    "from SentenceParcerForTenses import tense_predictorH, sentence_parser, sentence_adder\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readign and Prepearing The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruuf0aDblY0a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('gdrive/My Drive/Tooth Identifier/SentenceToTenses.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 99300,
     "status": "ok",
     "timestamp": 1568372013193,
     "user": {
      "displayName": "Mert Yilmaz Ertugrul (Student)",
      "photoUrl": "",
      "userId": "05571684688610331947"
     },
     "user_tz": -60
    },
    "id": "w1GYXoXVk0Vi",
    "outputId": "19035dc3-72d2-4fe1-af2c-607c5203ba2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.21.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "df.drop(['Unnamed: 0'], inplace=True, axis=1)\n",
    "df_columns = df.columns.tolist()\n",
    "columnsForDummies = df_columns[1:len(df_columns) - 1]\n",
    "dfd = pd.get_dummies(df, drop_first=True, columns=columnsForDummies)\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "df['tense'] = label_encoder.fit_transform(df['tense'])\n",
    "dt = joblib.load('gdrive/My Drive/Tooth Identifier/tense_dt_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 299182,
     "status": "ok",
     "timestamp": 1568372213089,
     "user": {
      "displayName": "Mert Yilmaz Ertugrul (Student)",
      "photoUrl": "",
      "userId": "05571684688610331947"
     },
     "user_tz": -60
    },
    "id": "sUplooaUlTHm",
    "outputId": "bffdd3d9-de0d-49a3-ef80-3eec833576e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "nlp = spacy.cli.download('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "47dcM-2ElbcQ"
   },
   "outputs": [],
   "source": [
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Creating Prediction Data Frame and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The functions below are also in the file SentenceParserForTenses.py, I re-define the functions because I like to change the functions on the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1WN_ck6jlgJh"
   },
   "outputs": [],
   "source": [
    "advance_vocab_df = pd.read_csv('gdrive/My Drive/Tooth Identifier/vocabulary.csv')  # Advanced words\n",
    "common_vocab_df = pd.read_csv('gdrive/My Drive/Tooth Identifier/common.csv')  # Common Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhzZxvpPliUH"
   },
   "outputs": [],
   "source": [
    "def syllables(word):\n",
    "    \"\"\"\n",
    "    returns the number of syllables of a given word\n",
    "    :param word: single word in string format\n",
    "    :return: number of syllables\n",
    "    \"\"\"\n",
    "    syllable_count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    if word[0] in vowels:\n",
    "        syllable_count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            syllable_count += 1\n",
    "    if word.endswith('e'):\n",
    "        syllable_count -= 1\n",
    "    if word.endswith('le') and len(word) > 2 and word[-3] not in vowels:\n",
    "        syllable_count += 1\n",
    "    if syllable_count == 0:\n",
    "        syllable_count += 1\n",
    "    return syllable_count\n",
    "\n",
    "\n",
    "def check_verb(token):\n",
    "    \"\"\"Check verb type given spacy token\n",
    "    :param token: single verb in a sting format\n",
    "    :return: linguistic type of verb\n",
    "    \"\"\"\n",
    "    indirect_object = False\n",
    "    direct_object = False\n",
    "    if token.tag_ == 'BES':\n",
    "        return 'TOBEVERB'\n",
    "    for item in token.children:\n",
    "        if item.dep_ == \"iobj\" or item.dep_ == \"pobj\":\n",
    "            indirect_object = True\n",
    "        if item.dep_ == \"dobj\" or item.dep_ == \"dative\":\n",
    "            direct_object = True\n",
    "    if indirect_object and direct_object:\n",
    "        return 'DITRANVERB'\n",
    "    elif direct_object and not indirect_object:\n",
    "        return 'TRANVERB'\n",
    "    elif not direct_object and not indirect_object:\n",
    "        return 'INTRANVERB'\n",
    "    else:\n",
    "        return 'VERB'\n",
    "\n",
    "\n",
    "def word_finder(vocab, word):\n",
    "    \"\"\"\n",
    "    Finds the verb in a given vocabulary\n",
    "    :param vocab: vocabulary in dataformat\n",
    "    :param word: word to look in vocabulary list\n",
    "    :return: boolean, whether the word in the vocabulary or not\n",
    "    \"\"\"\n",
    "    doc = nlp(word)\n",
    "    if doc[0].is_punct is False:\n",
    "        try:\n",
    "            result = vocab.word.str.contains(r'(?:\\s|^)' + word + '(?:\\s|$)').any()\n",
    "        except:\n",
    "            result = False\n",
    "        if result:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_advanced(word):\n",
    "    \"\"\"\n",
    "    Checks the word is in advance vocabulary\n",
    "    :return: boolean\n",
    "    :param word: word to look at\n",
    "    \"\"\"\n",
    "    return word_finder(advance_vocab_df, word)\n",
    "\n",
    "\n",
    "def is_common(word):\n",
    "    \"\"\"\n",
    "    Checks the word is in common vocabulary\n",
    "    :param word: word to look at\n",
    "    :return: boolean\n",
    "    \"\"\"\n",
    "    return word_finder(common_vocab_df, word)\n",
    "\n",
    "\n",
    "def feature_computer(doc):\n",
    "    \"\"\"\n",
    "    Creates and calculate features of the article\n",
    "    :param doc: tokenized article, spacy object\n",
    "    :return: dictionary of the features\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    # Linguistic\n",
    "    c_syllable = 0\n",
    "    c_monosylable = 0\n",
    "    c_disyllable = 0\n",
    "    c_complex_word = 0\n",
    "    c_word = 0\n",
    "    c_sentence = 0\n",
    "    c_character = 0\n",
    "    c_advance = 0\n",
    "    c_common = 0\n",
    "    c_paragraph = 0\n",
    "    # Word Usage\n",
    "    c_ditransverb = 0\n",
    "    c_transverb = 0\n",
    "    c_intransverb = 0\n",
    "    c_verb = 0\n",
    "    c_conjunction = 0\n",
    "    c_auxverb = 0\n",
    "    c_noun = 0\n",
    "    c_punct = 0\n",
    "    c_adjective = 0\n",
    "    c_adposition = 0\n",
    "    c_adverb = 0\n",
    "    c_pronoun = 0\n",
    "    c_unknown = 0\n",
    "    # Tenses\n",
    "    c_present_simple = 0\n",
    "    c_past_simple = 0\n",
    "    c_future_simple = 0\n",
    "    c_past_progressive = 0\n",
    "    c_present_progressive = 0\n",
    "    c_future_progressive = 0\n",
    "    c_present_perfect = 0\n",
    "    c_past_perfect = 0\n",
    "    c_future_perfect = 0\n",
    "    c_present_perfect_progressive = 0\n",
    "    c_past_perfect_progressive = 0\n",
    "    c_future_perfect_progressive = 0\n",
    "    tense_passed = False\n",
    "    c_tense_passed = 1\n",
    "\n",
    "    for sent in tqdm_notebook(doc.sents, total=len(list(doc.sents))):\n",
    "        c_sentence += 1\n",
    "        try:\n",
    "            parsed = sentence_parser(sent.text, print_sent=False)\n",
    "        except Exception as e:\n",
    "            exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "            print(exc_type, fname, exc_tb.tb_lineno, ' - ', c_tense_passed)\n",
    "            print(str(e))\n",
    "            tense_passed = True\n",
    "            c_tense_passed += 1\n",
    "            pass\n",
    "        if tense_passed is False:\n",
    "            tense = tense_predictorH(parsed, df, dfd, dt, label_encoder)\n",
    "            if tense == 'present simple':\n",
    "                c_present_simple += 1\n",
    "            if tense == 'past simple':\n",
    "                c_past_simple += 1\n",
    "            if tense == 'future simple':\n",
    "                c_future_simple += 1\n",
    "            if tense == 'past progressive':\n",
    "                c_past_progressive += 1\n",
    "            if tense == 'present progressive':\n",
    "                c_present_progressive += 1\n",
    "            if tense == 'future progressive':\n",
    "                c_future_progressive += 1\n",
    "            if tense == 'present perfect':\n",
    "                c_past_perfect += 1\n",
    "            if tense == 'present perfect progressive':\n",
    "                c_present_perfect_progressive += 1\n",
    "            if tense == 'past perfect progressive':\n",
    "                c_past_perfect_progressive += 1\n",
    "            if tense == 'future perfect progressive':\n",
    "                c_future_perfect_progressive += 1\n",
    "            if tense == 'future perfect':\n",
    "                c_future_perfect += 1\n",
    "            if tense == 'past perfect':\n",
    "                c_past_perfect += 1\n",
    "\n",
    "        for token in sent:\n",
    "            if token.is_stop is False and token.is_punct is False:\n",
    "                # Number of words\n",
    "                c_word += 1\n",
    "                # Number of characters\n",
    "                c_character += len(token.text)\n",
    "                # Syllables\n",
    "                num_syllable = syllables(token.text)\n",
    "                if num_syllable == 1:\n",
    "                    c_monosylable += 1\n",
    "                if num_syllable == 2:\n",
    "                    c_disyllable += 1\n",
    "                if num_syllable >= 3:\n",
    "                    c_complex_word += 1\n",
    "                c_syllable += num_syllable\n",
    "                # Advance Word\n",
    "                if is_advanced(token.text):\n",
    "                    c_advance += 1\n",
    "                # Common Word\n",
    "                if is_common(token.text):\n",
    "                    c_common += 1\n",
    "                # Paragraph\n",
    "                if token.is_space:\n",
    "                    c_paragraph += 1\n",
    "                # Word Usage\n",
    "                if token.pos_ == 'VERB':\n",
    "                    verb = check_verb(token)\n",
    "                    if verb == 'DITRANVERB':\n",
    "                        c_ditransverb += 1\n",
    "                    elif verb == 'TRANVERB':\n",
    "                        c_transverb += 1\n",
    "                    elif verb == 'INTRANVERB':\n",
    "                        c_intransverb += 1\n",
    "                    c_verb += 1\n",
    "                elif token.tag_ == 'CC' or token.pos_ == 'CCONJ' or token.pos_ == 'CONJ' or token.pos_ == 'SCONJ':\n",
    "                    c_conjunction += 1\n",
    "                elif token.pos_ == 'AUX':\n",
    "                    c_auxverb += 1\n",
    "                elif token.pos_ == 'NOUN':\n",
    "                    c_noun += 1\n",
    "                elif token.pos_ == 'PUNCT':\n",
    "                    c_punct += 1\n",
    "                elif token.pos_ == 'ADJ':\n",
    "                    c_adjective += 1\n",
    "                elif token.pos_ == 'ADP':\n",
    "                    c_adposition += 1\n",
    "                elif token.pos_ == 'ADV':\n",
    "                    c_adverb += 1\n",
    "                elif token.pos_ == 'PRON' or token.pos_ == 'PROPN':\n",
    "                    c_pronoun += 1\n",
    "                elif token.pos_ == 'X':\n",
    "                    c_unknown += 1\n",
    "        c_tense_passed = False\n",
    "\n",
    "    # Score calculating\n",
    "    dale_chall_score = (0.1579 * (100 - (c_common / c_word * 100))) + (0.0496 * (c_word / c_sentence))\n",
    "    Flesch_Reading_Ease_Score = 206.835 - (1.015 * c_word / c_sentence) - (84.6 * c_syllable / c_word)\n",
    "    new_flesch_reading_ease_score = (1.599 * c_monosylable / c_word * 100) - (1.015 * c_word / c_sentence) - 31.517\n",
    "    gunning_fog_score = 0.4 * ((c_word / c_sentence) + ((c_complex_word/c_word) * 100))\n",
    "    smog_score = 3 + sqrt((c_disyllable + c_complex_word) / c_word * 30)\n",
    "    forcast_score = 20 - (((c_monosylable / c_word) * 150)/10)\n",
    "    ari_score = 4.71 * (c_character / c_word) + (0.5 * (c_word / c_sentence)) - 21.43\n",
    "    coleman_liau_score = (0.0588 * c_character / c_word * 100) - (0.296 * (c_sentence / c_word) * 100) - 15.8\n",
    "    lix_score = (c_word / c_sentence) + (c_complex_word / c_word * 100)\n",
    "    rix_score = ((c_complex_word / c_word) * 100) + (c_word / c_sentence)\n",
    "    powers_sumner_kearl = (0.0778 * (c_word / c_sentence)) + (0.0455 * (c_syllable/c_word)*100) + 2.7971\n",
    "    spache_score = (0.121 * c_word / c_sentence) + (0.082 * (100 - (c_common / c_word) * 100)) + 0.659\n",
    "    linsear_write = (((c_disyllable * 2) + (c_complex_word * 3) / c_word) * 100)/10000\n",
    "\n",
    "    # Linguistic Features to Dictionary\n",
    "\n",
    "    result['Number of Sentence'] = c_sentence\n",
    "    result['Number of Words'] = c_word\n",
    "    result['Number of Paragraph'] = c_paragraph\n",
    "    try:\n",
    "        result['Words per Sentence (%)'] = c_word / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Words per Sentence (%)'] = 0\n",
    "    result['Number of Characters'] = c_character\n",
    "    try:\n",
    "        result['Character per Words (%)'] = c_character / c_word * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Character per Words (%)'] = 0\n",
    "    try:\n",
    "        result['Sentence per Paragraph (%)'] = c_sentence / c_paragraph * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Sentence per Paragraph (%)'] = 0\n",
    "    result['Number of Sylables'] = c_syllable\n",
    "    try:\n",
    "        result['Syllables per Word (%)'] = c_syllable / c_word * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Syllables per Word (%)'] = 0\n",
    "    result['Number of Monosyllable'] = c_monosylable\n",
    "    try:\n",
    "        result['Monosyllable per Word (%)'] = c_monosylable / c_word * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Monosyllable per Word (%)'] = 0\n",
    "    try:\n",
    "        result['Disyllable per Word (%)'] = c_disyllable / c_word * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Disyllable per Word (%)'] = 0\n",
    "    result['Number of Complex Words'] = c_complex_word\n",
    "    try:\n",
    "        result['Complex Words per Word (%)'] = c_complex_word / c_word * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Complex Words per Word (%)'] = 0\n",
    "    result['Number of Advance Words'] = c_advance\n",
    "    try:\n",
    "        result['Advance Words per Words (%)'] = c_advance / c_word * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Advance Words per Words (%)'] = 0\n",
    "    result['Number of Common Words'] = c_common\n",
    "    try:\n",
    "        result['Common Words per Words (%)'] = c_common / c_word * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Common Words per Words (%)'] = 0\n",
    "    try:\n",
    "        result['Verbs per Words (%)'] = c_verb / c_word * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Verbs per Words (%)'] = 0\n",
    "    try:\n",
    "        result['Verbs per Sentences (%)'] = c_verb / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Verbs per Sentence (%)'] = 0\n",
    "    try:\n",
    "        result['Ditransverbs per Sentences (%)'] = c_ditransverb / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Ditransverbs per Verbs (%)'] = 0\n",
    "    try:\n",
    "        result['Transverbs per Sentences (%)'] = c_transverb / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Transverbs per Sentences (%)'] = 0\n",
    "    try:\n",
    "        result['Intransitverbs per Sentences (%)'] = c_intransverb / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Intransitverbs per Sentences (%)'] = 0\n",
    "    try:\n",
    "        result['Auxverbs per Sentences (%)'] = c_auxverb / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Auxverbs per Sentences (%)'] = 0\n",
    "    try:\n",
    "        result['Conjuctions per Sentences (%)'] = c_conjunction / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Conjuctions per Sentences (%)'] = 0\n",
    "    try:\n",
    "        result['Nouns per Words (%)'] = c_noun / c_word * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Nouns per Words (%)'] = 0\n",
    "    try:\n",
    "        result['Nouns per Sentence (%)'] = c_noun / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Nouns per Sentence (%)'] = 0\n",
    "    try:\n",
    "        result['Puctuation per Sentence'] = c_punct / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Puctuation per Sentence'] = 0\n",
    "    try:\n",
    "        result['Adjective per Word (%)'] = c_adjective / c_word * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Adjective per Word (%)'] = 0\n",
    "    try:\n",
    "        result['Adjective per Sentence (%)'] = c_adjective / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Adjective per Sentence (%)'] = 0\n",
    "    try:\n",
    "        result['Adpositions per Sentences (%)'] = c_adposition / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Adpositions per Sentences (%)'] = 0\n",
    "    try:\n",
    "        result['Adverb per Sentence (%)'] = c_adverb / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Adverb per Sentence (%)'] = 0\n",
    "    try:\n",
    "        result['Pronoun per Sentence (%)'] = c_pronoun / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Pronoun per Sentence (%)'] = 0\n",
    "    result['Number of Unknown Words'] = c_unknown\n",
    "    try:\n",
    "        result['Unknown Words per Words (%)'] = c_unknown / c_word * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Unknown Words per Words (%)'] = 0\n",
    "\n",
    "    # Scores to dictionary\n",
    "    result['Dale Chall Score'] = dale_chall_score\n",
    "    result['Flesch Reading Ease Score'] = Flesch_Reading_Ease_Score\n",
    "    result['New Flesch Reading Ease Score'] = new_flesch_reading_ease_score\n",
    "    result['Gunning Fog Score'] = gunning_fog_score\n",
    "    result['SMOG Score'] = smog_score\n",
    "    result['FORCAST Score'] = forcast_score\n",
    "    result['ARI Score'] = ari_score\n",
    "    result['Coleman Liau Score'] = coleman_liau_score\n",
    "    result['LIX Score'] = lix_score\n",
    "    result['RIX Score'] = rix_score\n",
    "    result['Powers Sumner Kearl'] = powers_sumner_kearl\n",
    "    result['Spache Score'] = spache_score\n",
    "    result['Linsear Write'] = linsear_write\n",
    "\n",
    "    # Tenses to Dictionary\n",
    "    try:\n",
    "        result['Future Perfect Progressive (%)'] = c_future_perfect_progressive / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Future Perfect Progressive  (%)'] = 0\n",
    "    try:\n",
    "        result['Past Simple (%)'] = c_past_simple / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Past Simple (%)'] = 0\n",
    "    try:\n",
    "        result['Present Perfect Progressive (%)'] = c_present_perfect_progressive / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Present Perfect Progressive (%)'] = 0\n",
    "    try:\n",
    "        result['Future Progressive (%)'] = c_future_progressive / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Future Progressive (%)'] = 0\n",
    "    try:\n",
    "        result['Past Perfect Progressive (%)'] = c_past_perfect_progressive / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Past Perfect Progressive (%)'] = 0\n",
    "    try:\n",
    "        result['Past Progressive (%)'] = c_past_progressive / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Past Progressive (%)'] = 0\n",
    "    try:\n",
    "        result['Future Simple (%)'] = c_future_simple / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Future Simple (%)'] = 0\n",
    "    try:\n",
    "        result['Future Perfect (%)'] = c_future_perfect / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Future Perfect (%)'] = 0\n",
    "    try:\n",
    "        result['Present Simple (%)'] = c_present_simple / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Present Simple (%)'] = 0\n",
    "    try:\n",
    "        result['Past Perfect (%)'] = c_past_perfect / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Past Perfect (%)'] = 0\n",
    "    try:\n",
    "        result['Present Progressive (%)'] = c_present_progressive / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Present Progressive (%)'] = 0\n",
    "    try:\n",
    "        result['Present Perfect (%)'] = c_present_perfect / c_sentence * 100\n",
    "    except ZeroDivisionError:\n",
    "        result['Present Perfect (%)'] = 0\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def feature_computer_df(dic):\n",
    "    \"\"\"\n",
    "    Creates dataframe of the feature dictionary\n",
    "    :param dic: Features of the article in dictionary format\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    dictionary = feature_computer(dic)\n",
    "    return pd.DataFrame(dictionary, index=[0])\n",
    "\n",
    "\n",
    "def add_article(name, text, level, csv_file=None, is_pdf=False, num_col=False, print_it=True):\n",
    "    \"\"\"\n",
    "    Insert any sentence with proper tense tag to .csv file\n",
    "    :param is_pdf: True if the text is pdf\n",
    "    :param name: Name of the article\n",
    "    :param print_it: If true prints features of the document\n",
    "    :param num_col: If there is a column for row numbers in the csv files make it True\n",
    "    :param text: Text file to added\n",
    "    :param level: level of the sentence\n",
    "    :param csv_file: .csv file to save to\n",
    "    \"\"\"\n",
    "    if is_pdf is False:\n",
    "        with codecs.open(text, 'r', encoding='utf-8',\n",
    "                         errors='ignore') as myfile:\n",
    "            data = str(myfile.read()).replace('.', ' . ')\n",
    "            data = data.replace('-', '')\n",
    "            doc = nlp(data)\n",
    "    else:\n",
    "        pdf_file_obj = open(text, 'rb')\n",
    "        pdf_reader = PyPDF2.PdfFileReader(pdf_file_obj)\n",
    "\n",
    "        text = ''\n",
    "\n",
    "        for page in range(pdf_reader.numPages):\n",
    "            page_obj = pdf_reader.getPage(page)\n",
    "            text += page_obj.extractText()\n",
    "        doc = nlp(text)\n",
    "\n",
    "    new_article_df = feature_computer_df(doc)\n",
    "    new_article_df['Level'] = level\n",
    "    new_article_df.insert(0, 'Name', name)\n",
    "    data_df = pd.read_csv(csv_file)\n",
    "    if num_col:\n",
    "        data_df = df.drop(df.columns[0], axis=1)  # drop number column\n",
    "    data_df = data_df.append(new_article_df)\n",
    "    data_df.to_csv(csv_file, encoding='utf-8', index=False)\n",
    "    if print_it:\n",
    "        display('New data frame added')\n",
    "        display(data_df.tail(1))\n",
    "        # print(data_df.info())\n",
    "    else:\n",
    "        print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wzzVO7ZrlmG-"
   },
   "outputs": [],
   "source": [
    "def article_walker(title, text_path, dif_level,pdf=True,data_file = 'ArticleData.csv'):\n",
    "    add_article(name=title,text=text_path, level=dif_level, csv_file=data_file, is_pdf=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 729
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2698003,
     "status": "ok",
     "timestamp": 1568374611958,
     "user": {
      "displayName": "Mert Yilmaz Ertugrul (Student)",
      "photoUrl": "",
      "userId": "05571684688610331947"
     },
     "user_tz": -60
    },
    "id": "6AbDtopplwuq",
    "outputId": "40c72c02-00da-4604-bb7c-397ef58ba655"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e200bf7a634d0398621c6d41730671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J.K.-Rowling-Harry-Potter-and-the-Sorcerer_s-Stone-_Book-1_-Scholastic-Paperbacks-_1999_ skipped\n",
      "Fielding Helen. - Bridget Jones’s Diary (Филдинг Хелен. Дневник Бриджит Джонс) skipped\n",
      "Elizabeth-Gilbert-Eat_-Pray_-Love_-One-Woman_s-Search-for-Everything-Across-Italy_-India-and-Indones skipped\n",
      "J.K.-Rowling-Harry-Potter-And-The-Chamber-Of-Secrets-_Turtleback-School-_-Library-Binding-Edition_ skipped\n",
      "Edith-Eig_-Caroline-Greeven-Mother-of-Purl_-Friends_-Fun_-and-Fabulous-Designs-at-Hollywood_s-Knitti skipped\n",
      "Ethan-Nichtern-The-Dharma-of-The-Princess-Bride_-What-the-Coolest-Fairy-Tale-of-Our-Time-Can-Teach-U skipped\n",
      "Jodi Picoult The Pact_ A Love Story ||| gdrive/My Drive/Tooth Identifier/Easy Reading/Level 5/Jodi-Picoult-The-Pact_-A-Love-Story.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391d03ac97624fedb85d228538dc825c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=22678), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'IndexError'> <ipython-input-12-1db4abd0f808> 137  -  False\n",
      "list assignment index out of range\n",
      "<class 'IndexError'> <ipython-input-12-1db4abd0f808> 137  -  False\n",
      "list assignment index out of range\n",
      "<class 'IndexError'> <ipython-input-12-1db4abd0f808> 137  -  False\n",
      "list assignment index out of range\n",
      "<class 'IndexError'> <ipython-input-12-1db4abd0f808> 137  -  False\n",
      "list assignment index out of range\n",
      "<class 'IndexError'> <ipython-input-12-1db4abd0f808> 137  -  False\n",
      "list assignment index out of range\n",
      "<class 'IndexError'> <ipython-input-12-1db4abd0f808> 137  -  False\n",
      "list assignment index out of range\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'New data frame added'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Number of Sentence</th>\n",
       "      <th>Number of Words</th>\n",
       "      <th>Number of Paragraph</th>\n",
       "      <th>Words per Sentence (%)</th>\n",
       "      <th>Number of Characters</th>\n",
       "      <th>Character per Words (%)</th>\n",
       "      <th>Sentence per Paragraph (%)</th>\n",
       "      <th>Number of Sylables</th>\n",
       "      <th>Syllables per Word (%)</th>\n",
       "      <th>Number of Monosyllable</th>\n",
       "      <th>Monosyllable per Word (%)</th>\n",
       "      <th>Disyllable per Word (%)</th>\n",
       "      <th>Number of Complex Words</th>\n",
       "      <th>Complex Words per Word (%)</th>\n",
       "      <th>Number of Advance Words</th>\n",
       "      <th>Advance Words per Words (%)</th>\n",
       "      <th>Number of Common Words</th>\n",
       "      <th>Common Words per Words (%)</th>\n",
       "      <th>Verbs per Words (%)</th>\n",
       "      <th>Verbs per Sentences (%)</th>\n",
       "      <th>Ditransverbs per Sentences (%)</th>\n",
       "      <th>Transverbs per Sentences (%)</th>\n",
       "      <th>Intransitverbs per Sentences (%)</th>\n",
       "      <th>Auxverbs per Sentences (%)</th>\n",
       "      <th>Conjuctions per Sentences (%)</th>\n",
       "      <th>Nouns per Words (%)</th>\n",
       "      <th>Nouns per Sentence (%)</th>\n",
       "      <th>Puctuation per Sentence</th>\n",
       "      <th>Adjective per Word (%)</th>\n",
       "      <th>Adjective per Sentence (%)</th>\n",
       "      <th>Adpositions per Sentences (%)</th>\n",
       "      <th>Adverb per Sentence (%)</th>\n",
       "      <th>Pronoun per Sentence (%)</th>\n",
       "      <th>Number of Unknown Words</th>\n",
       "      <th>Unknown Words per Words (%)</th>\n",
       "      <th>Dale Chall Score</th>\n",
       "      <th>Flesch Reading Ease Score</th>\n",
       "      <th>New Flesch Reading Ease Score</th>\n",
       "      <th>Gunning Fog Score</th>\n",
       "      <th>SMOG Score</th>\n",
       "      <th>FORCAST Score</th>\n",
       "      <th>ARI Score</th>\n",
       "      <th>Coleman Liau Score</th>\n",
       "      <th>LIX Score</th>\n",
       "      <th>RIX Score</th>\n",
       "      <th>Powers Sumner Kearl</th>\n",
       "      <th>Spache Score</th>\n",
       "      <th>Linsear Write</th>\n",
       "      <th>Future Perfect Progressive (%)</th>\n",
       "      <th>Past Simple (%)</th>\n",
       "      <th>Present Perfect Progressive (%)</th>\n",
       "      <th>Future Progressive (%)</th>\n",
       "      <th>Past Perfect Progressive (%)</th>\n",
       "      <th>Past Progressive (%)</th>\n",
       "      <th>Future Simple (%)</th>\n",
       "      <th>Future Perfect (%)</th>\n",
       "      <th>Present Simple (%)</th>\n",
       "      <th>Past Perfect (%)</th>\n",
       "      <th>Present Progressive (%)</th>\n",
       "      <th>Present Perfect (%)</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jodi-Picoult-The-Pact_-A-Love-Story</td>\n",
       "      <td>22678</td>\n",
       "      <td>76817</td>\n",
       "      <td>12996</td>\n",
       "      <td>338.729165</td>\n",
       "      <td>388049</td>\n",
       "      <td>505.160316</td>\n",
       "      <td>174.499846</td>\n",
       "      <td>123171</td>\n",
       "      <td>160.343414</td>\n",
       "      <td>43279</td>\n",
       "      <td>56.340393</td>\n",
       "      <td>30.700236</td>\n",
       "      <td>9955</td>\n",
       "      <td>12.959371</td>\n",
       "      <td>1511</td>\n",
       "      <td>1.967013</td>\n",
       "      <td>28369</td>\n",
       "      <td>36.930627</td>\n",
       "      <td>28.198185</td>\n",
       "      <td>95.515478</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>31.215275</td>\n",
       "      <td>64.189964</td>\n",
       "      <td>0.154335</td>\n",
       "      <td>0.026457</td>\n",
       "      <td>30.192536</td>\n",
       "      <td>102.270923</td>\n",
       "      <td>0.00441</td>\n",
       "      <td>7.37467</td>\n",
       "      <td>24.980157</td>\n",
       "      <td>2.623688</td>\n",
       "      <td>12.673075</td>\n",
       "      <td>39.324455</td>\n",
       "      <td>10</td>\n",
       "      <td>0.013018</td>\n",
       "      <td>10.126664</td>\n",
       "      <td>67.746371</td>\n",
       "      <td>55.133188</td>\n",
       "      <td>6.538665</td>\n",
       "      <td>6.6191</td>\n",
       "      <td>11.548941</td>\n",
       "      <td>4.056697</td>\n",
       "      <td>5.164882</td>\n",
       "      <td>16.346663</td>\n",
       "      <td>16.346663</td>\n",
       "      <td>10.356257</td>\n",
       "      <td>6.240551</td>\n",
       "      <td>471.663888</td>\n",
       "      <td>0.017638</td>\n",
       "      <td>2.332657</td>\n",
       "      <td>0.00441</td>\n",
       "      <td>0.017638</td>\n",
       "      <td>0.383632</td>\n",
       "      <td>0.740806</td>\n",
       "      <td>0.410089</td>\n",
       "      <td>0.132287</td>\n",
       "      <td>6.61434</td>\n",
       "      <td>2.143046</td>\n",
       "      <td>0.149925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Name  ...  Level\n",
       "0  Jodi-Picoult-The-Pact_-A-Love-Story  ...      5\n",
       "\n",
       "[1 rows x 62 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barbara-Kingsolver-The-Poisonwood-Bible-Harper-Perennial-Modern-Classics-_2005_ skipped\n",
      "Maeve-Binchy-Circle-of-Friends-Dell-_2007_ skipped\n",
      "Margaret Mitchell Gone with the Wind Warner Books _1936_ ||| gdrive/My Drive/Tooth Identifier/Easy Reading/Level 5/Margaret-Mitchell-Gone-with-the-Wind-Warner-Books-_1936_.txt\n",
      "Margaret-Mitchell-Gone-with-the-Wind-Warner-Books-_1936_ gdrive/My Drive/Tooth Identifier/Easy Reading/Level 5/Margaret-Mitchell-Gone-with-the-Wind-Warner-Books-_1936_.txt  has FAILD [E088] Text of length 2366110 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n",
      "Charlotte Brontë Jane Eyre HarperCollins Publishers _2011_ ||| gdrive/My Drive/Tooth Identifier/Easy Reading/Level 5/Charlotte-Brontë-Jane-Eyre-HarperCollins-Publishers-_2011_.txt\n",
      "Charlotte-Brontë-Jane-Eyre-HarperCollins-Publishers-_2011_ gdrive/My Drive/Tooth Identifier/Easy Reading/Level 5/Charlotte-Brontë-Jane-Eyre-HarperCollins-Publishers-_2011_.txt  has FAILD [E088] Text of length 1058864 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = 'gdrive/My Drive/Tooth Identifier/Easy Reading/Level 5'\n",
    "level = 5\n",
    "file_name ='gdrive/My Drive/Tooth Identifier/ArticleData4Unlabeled.csv'\n",
    "\n",
    "data_df = pd.read_csv(file_name)\n",
    "files = []\n",
    "file_names = []\n",
    "file_dict ={}\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.txt' in file:\n",
    "            file_dict[file] = os.path.join(r, file)\n",
    "            files.append(os.path.join(r, file))\n",
    "            file_names.append(file)\n",
    "\n",
    "for k,v in tqdm_notebook(file_dict.items(), total=len(file_dict.items())):\n",
    "    f_name = k.replace('.txt','').strip()\n",
    "    if not f_name in data_df.Name.unique():\n",
    "      f_name = f_name.replace(',', '')\n",
    "      f_name = f_name.replace('-', ' ')\n",
    "      try:\n",
    "          print(f_name, '|||', v.strip())\n",
    "          article_walker(k.replace('.txt','').strip(), v.strip(), dif_level=level,pdf=False,data_file = file_name)\n",
    "      except Exception as e:\n",
    "          print(k.replace('.txt','').strip(), v.strip(), ' has FAILD', str(e))\n",
    "    else:\n",
    "      print(f_name, 'skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_Ier-ybmZts"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Text Comlexity Colab 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
